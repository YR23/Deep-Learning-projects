{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd \nfrom keras.utils import np_utils\nimport os\nimport keras\nimport gensim\nfrom random import randint\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom keras.utils import to_categorical\nfrom numpy import argmax\nfrom keras import Sequential\nfrom keras.layers import LSTM,Input,Lambda,concatenate, Dense,Dropout,GRU,BatchNormalization\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text as keras_text, sequence as keras_seq\nimport re\nimport keras.backend as K\nfrom keras.models import Model\nfrom keras.optimizers import Adadelta\nfrom time import time\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nimport tensorflow as tf\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split,StratifiedKFold",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29e2950b03297762e4548ac71b866807fc77b5ce"
      },
      "cell_type": "code",
      "source": "def ReadData():\n    train1 = pd.read_csv(\"../input/homedepot/train.csv\",encoding ='latin1')\n    test1 = pd.read_csv(\"../input/home-depot-product-search-relevance/test.csv\",encoding ='latin1')\n    return train1,test1\ndef ConcatDescAndTitle(df):\n    train1 = df.merge(desc,on='product_uid',how='left')\n    #train1['TitleAndDesc'] = train1[['product_title', 'product_description']].apply(lambda x: ''.join(x), axis=1)\n    train1 = train1.drop(columns=['product_description','id'])\n    #train1['product_title'] = train1['product_title'].str.replace('[^a-zA-Z0-9\\s]','$')\n    #train1['search_term'] = train1['search_term'].str.replace('[^a-zA-Z0-9\\s]','$')\n    train1['product_title'] = train1['product_title'].str.replace('\\\\xa0', '')\n    train1['search_term'] = train1['search_term'].str.replace('\\\\xa0', '')\n    return train1\ndef getBrandAndMatirial():\n    for i in range(test.shape[0]):\n        row = test.loc[i]\n        #foo = df.ix[(df['column1']==value) | (df['columns2'] == 'b') | (df['column3'] == 'c')]\n        row2 = attributes[attributes[\"product_uid\"]==row[\"product_uid\"]]\n        brand = row2[row2[\"name\"]==str(\"MFG Brand Name\")][\"value\"]\n        Material = row2[row2[\"name\"]==str(\"Material\")][\"value\"]\n        if ((brand.values.any())):\n            if ((Material.values.any())):\n                train.loc[i,\"product_title\"] += \" \" + str(brand.values[0]) + \" \" + str(Material.values[0])\n    return train\ndef RemoveNotCommonRelevances():\n    train2 = train\n    train2 = train2[train2.relevance != 1.25]\n    train2 = train2[train2.relevance != 1.5]\n    train2 = train2[train2.relevance != 2.5]\n    train2 = train2[train2.relevance != 1.75]\n    train2 = train2[train2.relevance != 2.5]\n    train2 = train2[train2.relevance != 2.25]\n    train2 = train2[train2.relevance != 2.75]\n    return train2.reset_index(drop=True)\ndef AddMoreIRelevance(train):\n    size = len(chars)\n    for i in range(40000):\n        if (i%5000==0):\n            print(i)\n        title = \"\"\n        term = \"\"\n        for j in range(67):\n            title += chars[randint(0, size-1)]\n            term += chars[randint(0, size-1)]\n        rander = randint(0, 1)\n        rel = 0\n        if (rander==0):\n            rel = 1.0\n        if (rander==1):\n             rel = 1.33\n        train = train.append({'product_title':title,'search_term':term,\"relevance\":rel }, ignore_index=True)\n    return train",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "chars = \"abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\nlenthofchar = len(chars)\ntrain,test = ReadData()\ntrain = RemoveNotCommonRelevances()",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc020c34a3f65283e3eb50cb868546e6c3d46c2d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "   id            ...                            search_term\n0   1            ...                      90 degree bracket\n1   4            ...                       metal l brackets\n2   5            ...                       simpson sku able\n3   6            ...                   simpson strong  ties\n4   7            ...              simpson strong tie hcc668\n\n[5 rows x 4 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>product_uid</th>\n      <th>product_title</th>\n      <th>search_term</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>100001</td>\n      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n      <td>90 degree bracket</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>100001</td>\n      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n      <td>metal l brackets</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>100001</td>\n      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n      <td>simpson sku able</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>100001</td>\n      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n      <td>simpson strong  ties</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>100001</td>\n      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n      <td>simpson strong tie hcc668</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a60bbb758c361613ebbd8557713299cb191bfa1"
      },
      "cell_type": "code",
      "source": "def CreateDic():\n    CharToIndex2 = {}\n    counter=0\n    for key in model.wv.vocab.keys():\n        CharToIndex2[key] = counter\n        counter += 1\n    return CharToIndex2\ndef CreateOneHot(index):\n    arr = np.zeros(lenthofchar)\n    if (index!=-1):\n        arr[index] = 1\n    return arr\ndef CreateCategorial(y):\n    UniquToPred2 = {}\n    PredToUnique2 = {}\n    u = np.unique(y)\n    for i in range(len(u)):\n        UniquToPred2[i] = u[i]\n        PredToUnique2[u[i]] = i\n    return UniquToPred2,PredToUnique2\ndef CreateOneHotLength(index,length):\n    arr = np.zeros(length)\n    arr[index] = 1\n    return arr\ndef CreateY(df):\n    y = np.zeros((df.shape[0],7))\n    for i in range(df.shape[0]):\n            y[i] = CreateOneHotLength(PredToUnique[df[i]],7)\n    return y",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a876241138b1aa4d5679b51f13496b6557482707"
      },
      "cell_type": "markdown",
      "source": "****Train Test Split with suffle****"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24074dde91a9b613247e6437eaba33116dfdbf11"
      },
      "cell_type": "code",
      "source": "def spliting(x,trainsize):\n    X_train, X_test = train_test_split(x, test_size=1-trainsize, shuffle=True)\n    X_train = X_train.reset_index()\n    X_train = X_train.drop(columns=['index'])\n    X_test = X_test.reset_index()\n    X_test = X_test.drop(columns=['index'])\n    return X_train, X_test,X_train.relevance,X_test.relevance\ndef LeftRightAndYTrain(df,y,index):\n    left = df[0][index:]\n    right = df[1][index:]\n    y = y[index]\n    y = CreateY(y)\n    return left,right,y\ndef LeftRightAndYTest(df,y,index):\n    left = df[0][index]\n    right = df[1][index]\n    y = y[index]\n    return left,right,y\ndef ReplaceCharsWithOneHotMaxLen(temp):\n    avglen = 0\n    for i in range(temp.shape[0]):\n        lis =(list(temp.loc[i][\"product_title\"]))\n        avglen += len(lis)\n    avglen = int(math.floor(avglen/temp.shape[0]))\n    avglen = 50\n    final_df = np.empty([2,temp.shape[0],avglen,lenthofchar])\n    for i in range(final_df.shape[1]):\n        lis =(list(temp.loc[i][\"product_title\"]))\n        lis2 =(list(temp.loc[i][\"search_term\"]))\n        for j in range(final_df.shape[2]):\n            if (j<len(lis)):\n                final_df[0,i,j]=CreateOneHot(chars.find(lis[j]))\n            else:\n                final_df[0,i,j]= CreateOneHot(-1)\n            if (j<len(lis2)):\n                final_df[1,i,j]= CreateOneHot(chars.find(lis2[j]))\n            else:\n                final_df[1,i,j]=CreateOneHot(-1)\n    return final_df\ndef ConveteBackToPred():\n    newPred = np.zeros(preds2.shape[0],)\n    for i in range(preds2.shape[0]):\n        maxim=float(0)\n        indx=0\n        for j in range(preds2[i].size):\n            if (preds[i][j]>maxim):\n                indx = j\n                maxim = preds2[i][j]\n        newPred[i] = UniquToPred[indx] \n    return newPred      ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5a43b67b9dcd8806377cceba6c3385402b02ca4"
      },
      "cell_type": "code",
      "source": "train1, test1,y_train,y_test = spliting(train,0.8)\nUniquToPred,PredToUnique = CreateCategorial(train[\"relevance\"].astype(str))\ny_train = y_train.values\ny_train = y_train.astype(str)\ny_train = CreateY(y_train)\ntrain1 = ReplaceCharsWithOneHotMaxLen(train1)\ntest1 = ReplaceCharsWithOneHotMaxLen(test1)\ntest = ReplaceCharsWithOneHotMaxLen(test)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88111ba52441464f9a43346d2878e51f589bce5b"
      },
      "cell_type": "markdown",
      "source": "**trying the Kfold**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "679644151c93c5310629c8ffac51179259d183b6"
      },
      "cell_type": "code",
      "source": "def cossim(left, right):\n    ans = (K.sum(left*right))/(K.sqrt(K.sum(K.pow(left,2)))*K.sqrt(K.sum(K.pow(right,2))))\n    ans = tf.nn.relu(ans)\n    return ans*2 +1\ndef custum_mean_squared_error(y_true, y_pred):\n    return K.mean(K.square(((y_pred - y_true)))+(y_pred - y_true)*margin, axis=-1)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e836b0c2872f8791a1b7954f8400712df673791b"
      },
      "cell_type": "code",
      "source": "def getModel():\n    left_input = Input(shape=(X_train_left_kfold.shape[1], X_train_left_kfold.shape[2]))\n    right_input = Input(shape=(X_train_right_kfold.shape[1], X_train_right_kfold.shape[2]))\n    shared_lstm = LSTM(n_hidden)\n    left_out = shared_lstm(left_input)\n    right_out = shared_lstm(right_input)\n    left_out_norm = BatchNormalization()(left_out)\n    right_out_norm = BatchNormalization()(right_out)\n    merged_vector = concatenate([left_out_norm, right_out_norm], axis=-1)\n    #malstm_distance = Lambda(function=lambda x: cossim(x[0], x[1]),output_shape=lambda x: (1, 1))([left_out, right_out])\n    predictions2 = Dense(20,activation=\"sigmoid\")(merged_vector)\n    predictions4 = Dense(7,activation=\"softmax\")(predictions2)\n    # Pack it all up into a model\n    malstm = Model([left_input, right_input], [predictions4])\n\n    #optimizer = Adadelta(clipnorm=1.25)\n    #malstm.compile(loss='mse', optimizer=\"adam\", metrics=['accuracy'])\n    #sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n\n    malstm.compile(loss='mse', optimizer=\"adam\")\n    malstm.summary()\n    return malstm",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c77f8c54461f8a117aa303bd34e72b32c3e34ab"
      },
      "cell_type": "markdown",
      "source": "**THE LSTM NetWork**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c621041127cb095f45e74bcd4bf08a58e629a4ff",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "n_hidden = 100\nbatch_size = 526\ntotal_rmse = 0\ntotal_mae = 0\nn_epoch = 3\nX_train_left_kfold = train1[0]\nX_train_right_kfold = train1[1]\nX_test_left_kfold = test1[0]\nX_test_right_kfold = test1[1]\nmalstm1 = getModel()\ntraining_start_time = time()\nmalstm_trained = malstm1.fit([X_train_left_kfold,X_train_right_kfold],y_train, batch_size=batch_size, nb_epoch=n_epoch,validation_split=0.05)\n#preds = malstm1.predict([X_test_left_kfold,X_test_right_kfold], batch_size=batch_size)\n#pred = ConveteBackToPred()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 50, 63)       0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 50, 63)       0                                            \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 100)          65600       input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 100)          400         lstm_1[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 100)          400         lstm_1[1][0]                     \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 200)          0           batch_normalization_1[0][0]      \n                                                                 batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 20)           4020        concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 7)            147         dense_1[0][0]                    \n==================================================================================================\nTotal params: 70,567\nTrainable params: 70,167\nNon-trainable params: 400\n__________________________________________________________________________________________________\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  if sys.path[0] == '':\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Train on 56245 samples, validate on 2961 samples\nEpoch 1/3\n56245/56245 [==============================] - 60s 1ms/step - loss: 0.1157 - val_loss: 0.1174\nEpoch 2/3\n56245/56245 [==============================] - 58s 1ms/step - loss: 0.1143 - val_loss: 0.1146\nEpoch 3/3\n56245/56245 [==============================] - 57s 1ms/step - loss: 0.1141 - val_loss: 0.1143\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1576de2947796f9fd86fe19c757d924887a22a30"
      },
      "cell_type": "code",
      "source": "preds2 = malstm1.predict([test[0],test[1]], batch_size=batch_size)\npred2 = ConveteBackToPred()\ntrain,test = ReadData()\nsample_sub = pd.DataFrame()\nsample_sub['id'] = test['id']\nsample_sub['relevance'] = pred2\nsample_sub.to_csv('charfinal.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5e80cbd6221ea532865993014c0e59a277ac4df"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}